# model_inferencer.py
import os
import time
import warnings
import torch
import onnxruntime as ort
import numpy as np
import cv2

warnings.filterwarnings('ignore')

HORROR_ONNX_PATH = "horror_model_static_quantized.onnx"
VIOLENCE_ONNX_PATH = "violence_model_static_quantized.onnx"
NSFW_ONNX_PATH = "nsfw_model_static_quantized.onnx"

_models = {}
_device_info = {}
_models_loaded = False  # æ·»åŠ æ¨¡å‹åŠ è½½æ ‡å¿—


def optimized_preprocess(frame_array):
    """ä¼˜åŒ–çš„é¢„å¤„ç†å‡½æ•°"""
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
        if isinstance(frame_array, list):
            frame_array = np.array(frame_array)

        # ç¡®ä¿æ˜¯uint8ç±»å‹
        if frame_array.dtype != np.uint8:
            frame_array = frame_array.astype(np.uint8)

        img = cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)
        img = img.astype(np.float32) / 255.0
        img = (img - 0.5) / 0.5
        img = np.transpose(img, (2, 0, 1))
        return np.expand_dims(img, axis=0)
    except Exception as e:
        print(f"é¢„å¤„ç†å¤±è´¥: {e}")
        # è¿”å›ä¸€ä¸ªé»˜è®¤çš„é¢„å¤„ç†ç»“æœ
        return np.zeros((1, 3, 224, 224), dtype=np.float32)


def load_all_models():
    """åŠ è½½æ‰€æœ‰ONNXæ¨¡å‹"""
    global _models, _device_info, _models_loaded

    if _models_loaded:
        return

    try:
        print("ğŸš€ æ­£åœ¨åŠ è½½ONNXæ¨¡å‹...")

        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] \
            if torch.cuda.is_available() else ['CPUExecutionProvider']

        print(f"ğŸ“¡ ä½¿ç”¨æ¨ç†åç«¯: {providers[0]}")

        # åŠ è½½æ¨¡å‹
        _models["horror"] = ort.InferenceSession(HORROR_ONNX_PATH, providers=providers)
        _models["violence"] = ort.InferenceSession(VIOLENCE_ONNX_PATH, providers=providers)
        _models["nsfw"] = ort.InferenceSession(NSFW_ONNX_PATH, providers=providers)

        # è·å–è¾“å…¥åç§°
        _models["horror_input"] = _models["horror"].get_inputs()[0].name
        _models["violence_input"] = _models["violence"].get_inputs()[0].name
        _models["nsfw_input"] = _models["nsfw"].get_inputs()[0].name

        # è·å–è¾“å‡ºåç§°
        _models["horror_output"] = _models["horror"].get_outputs()[0].name
        _models["violence_output"] = _models["violence"].get_outputs()[0].name
        _models["nsfw_output"] = _models["nsfw"].get_outputs()[0].name

        _device_info["onnxruntime_providers"] = providers
        _models_loaded = True

        print("âœ… ONNX æ¨¡å‹åŠ è½½å®Œæˆ")

        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œ
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...")
        test_input = np.random.randn(1, 3, 224, 224).astype(np.float32)
        try:
            _models["horror"].run(None, {_models["horror_input"]: test_input})
            print("âœ“ Horroræ¨¡å‹æµ‹è¯•é€šè¿‡")
            _models["violence"].run(None, {_models["violence_input"]: test_input})
            print("âœ“ Violenceæ¨¡å‹æµ‹è¯•é€šè¿‡")
            _models["nsfw"].run(None, {_models["nsfw_input"]: test_input})
            print("âœ“ NSFWæ¨¡å‹æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def infer_batch_from_memory(frame_list):
    """æ‰¹é‡æ¨ç†å‡½æ•°"""
    try:
        load_all_models()

        if not frame_list:
            print("âš ï¸ è­¦å‘Šï¼šä¼ å…¥çš„å¸§åˆ—è¡¨ä¸ºç©º")
            return []

        batch_size = len(frame_list)
        print(f"ğŸ” å¼€å§‹æ‰¹é‡æ¨ç†ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")

        # 1. æ‰¹é‡é¢„å¤„ç†
        preprocess_start = time.perf_counter()

        batch_inputs = []
        for frame in frame_list:
            try:
                input_tensor = optimized_preprocess(frame)
                batch_inputs.append(input_tensor)
            except Exception as e:
                print(f"âš ï¸ å•å¸§é¢„å¤„ç†å¤±è´¥: {e}")
                # ä½¿ç”¨é›¶å¼ é‡ä½œä¸ºå ä½ç¬¦
                batch_inputs.append(np.zeros((1, 3, 224, 224), dtype=np.float32))

        # åˆå¹¶æ‰¹æ¬¡
        try:
            batch_input = np.concatenate(batch_inputs, axis=0)
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡åˆå¹¶å¤±è´¥: {e}")
            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å…¥å¹¶è°ƒæ•´æ‰¹æ¬¡
            if batch_inputs:
                single_input = batch_inputs[0]
                batch_input = np.repeat(single_input, batch_size, axis=0)
            else:
                batch_input = np.zeros((batch_size, 3, 224, 224), dtype=np.float32)

        preprocess_time = (time.perf_counter() - preprocess_start) * 1000

        # 2. æ‰¹é‡æ¨ç†
        infer_start = time.perf_counter()

        try:
            # æ¨ç†ä¸‰ä¸ªæ¨¡å‹
            horror_outputs = _models["horror"].run(
                None, {_models["horror_input"]: batch_input}
            )[0]

            violence_outputs = _models["violence"].run(
                None, {_models["violence_input"]: batch_input}
            )[0]

            nsfw_outputs = _models["nsfw"].run(
                None, {_models["nsfw_input"]: batch_input}
            )[0]

        except Exception as e:
            print(f"âŒ æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            horror_outputs = np.zeros((batch_size, 2), dtype=np.float32)
            violence_outputs = np.zeros((batch_size, 2), dtype=np.float32)
            nsfw_outputs = np.zeros((batch_size, 2), dtype=np.float32)

        infer_time = (time.perf_counter() - infer_start) * 1000

        # 3. è®¡ç®—æ¦‚ç‡
        try:
            # ä½¿ç”¨ç¨³å®šçš„softmaxè®¡ç®—
            def stable_softmax(x):
                exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
                return exp_x / np.sum(exp_x, axis=1, keepdims=True)

            horror_probs = stable_softmax(horror_outputs)
            violence_probs = stable_softmax(violence_outputs)
            nsfw_probs = stable_softmax(nsfw_outputs)

        except Exception as e:
            print(f"âŒ æ¦‚ç‡è®¡ç®—å¤±è´¥: {e}")
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåå¤‡
            horror_probs = np.full((batch_size, 2), 0.5)
            violence_probs = np.full((batch_size, 2), 0.5)
            nsfw_probs = np.full((batch_size, 2), 0.5)

        # 4. æ„å»ºç»“æœ
        results = []
        for i in range(batch_size):
            try:
                # è·å–æ¦‚ç‡å€¼ï¼Œç¡®ä¿ç´¢å¼•æ­£ç¡®
                horror_score = float(horror_probs[i, 0] * 100)  # ç´¢å¼•0æ˜¯ææ€–ç±»åˆ«
                violence_score = float(violence_probs[i, 1] * 100)  # ç´¢å¼•1æ˜¯æš´åŠ›ç±»åˆ«
                nsfw_score = float(nsfw_probs[i, 1] * 100)  # ç´¢å¼•1æ˜¯è‰²æƒ…ç±»åˆ«

                result = {
                    "horror": round(horror_score, 2),
                    "violence": round(violence_score, 2),
                    "nsfw": round(nsfw_score, 2),
                    "performance": {
                        "batch_size": batch_size,
                        "preprocess_time_ms": round(preprocess_time / batch_size, 2),
                        "total_infer_time_ms": round(infer_time / batch_size, 2),
                        "batch_total_infer_time_ms": round(infer_time, 2),
                        "fps": round(batch_size * 1000 / max(infer_time, 0.001), 2),
                        "device": _device_info.get("onnxruntime_providers", ["CPU"])[0],
                        "mode": "batch",
                        "status": "success"
                    }
                }
                results.append(result)

            except Exception as e:
                print(f"âŒ ç¬¬{i}å¸§ç»“æœæ„å»ºå¤±è´¥: {e}")
                # è¿”å›é»˜è®¤ç»“æœ
                results.append({
                    "horror": 0.0,
                    "violence": 0.0,
                    "nsfw": 0.0,
                    "performance": {
                        "batch_size": batch_size,
                        "preprocess_time_ms": 0,
                        "total_infer_time_ms": 0,
                        "batch_total_infer_time_ms": 0,
                        "fps": 0,
                        "device": "Unknown",
                        "mode": "batch",
                        "status": "error",
                        "error": str(e)
                    }
                })

        print(
            f"âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œå¤„ç†äº†{len(results)}å¸§ï¼Œå¹³å‡FPS: {results[0]['performance']['fps'] if results else 0:.2f}")
        return results

    except Exception as e:
        print(f"âŒ æ‰¹é‡æ¨ç†å‡½æ•°æ•´ä½“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

        # è¿”å›ä¸€ä¸ªé»˜è®¤çš„ç»“æœåˆ—è¡¨
        return [{
            "horror": 0.0,
            "violence": 0.0,
            "nsfw": 0.0,
            "performance": {
                "batch_size": len(frame_list) if frame_list else 1,
                "preprocess_time_ms": 0,
                "total_infer_time_ms": 0,
                "batch_total_infer_time_ms": 0,
                "fps": 0,
                "device": "Unknown",
                "mode": "batch",
                "status": "critical_error",
                "error": str(e)
            }
        } for _ in range(len(frame_list) if frame_list else 1)]


def benchmark_inference(sample_size=100):
    """åŸºå‡†æµ‹è¯•å‡½æ•°"""
    print(f"ğŸ§ª å¼€å§‹åŸºå‡†æµ‹è¯•ï¼Œæ ·æœ¬æ•°: {sample_size}")

    # åˆ›å»ºæµ‹è¯•å¸§
    test_frames = []
    for i in range(sample_size):
        frame = np.random.randint(0, 256, (720, 970, 3), dtype=np.uint8)
        test_frames.append(frame)

    # æµ‹è¯•æ‰¹é‡æ¨ç†
    start_time = time.time()
    results = infer_batch_from_memory(test_frames)
    total_time = time.time() - start_time

    if results:
        avg_fps = sample_size / total_time
        avg_infer_time = total_time * 1000 / sample_size

        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(f"  å¤„ç†å¸§æ•°: {len(results)}")
        print(f"  æ€»æ—¶é—´: {total_time:.2f}ç§’")
        print(f"  å¹³å‡FPS: {avg_fps:.2f}")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {avg_infer_time:.2f}ms")

        return {
            "sample_size": sample_size,
            "total_time": round(total_time, 2),
            "avg_fps": round(avg_fps, 2),
            "avg_infer_time_ms": round(avg_infer_time, 2),
            "device_info": _device_info
        }

    return {"error": "åŸºå‡†æµ‹è¯•å¤±è´¥"}


if __name__ == "__main__":
    # æµ‹è¯•æ‰¹é‡æ¨ç†
    print("ğŸš€ æµ‹è¯•æ‰¹é‡æ¨ç†...")
    test_frame = np.random.randint(0, 256, (720, 970, 3), dtype=np.uint8)
    results = infer_batch_from_memory([test_frame])
    print(f"æµ‹è¯•ç»“æœ: {results}")